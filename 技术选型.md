# 总览（结论先行）

**首选技术栈（MVP）**

- 前端：React（或 Next.js） + Web Audio API + WebSocket（或 WebRTC）
- 网关 / 负载：Nginx / Traefik（反向代理、TLS） + WebSocket 路由
- Orchestrator（AI 调用层）：Python + FastAPI（Streaming 支持）
- 音频处理微服务（性能关键）：C++（libopus / libwebrtc / libhv）通过 gRPC 暴露接口
- LLM / STT / TTS：OpenAI 音频-enabled LLM（GPT-4o 系列）作主通道；备选 Anthropic（企业合规）
- 向量搜索/记忆：RedisVector / Milvus / Pinecone（任选）
- 数据库：PostgreSQL（用户/元数据） + Redis（会话缓存）
- 部署：Kubernetes（推荐）或 Docker Compose（小规模测试）
- CI/CD：GitHub Actions / GitLab CI
- 监控：Prometheus + Grafana；日志收集：ELK / Loki
- 安全：OAuth2 / JWT、API key 后端保管、内容审查流水线

你的角色建议：

- **你用 C++**：实现高性能音频编解码、降噪、VAD、opus 转码微服务（gRPC）
- **你用 Python**：实现 Orchestrator（LLM/STT/TTS 调用、prompt 管理、RAG、session）
- **你用 Java**（可选）：实现用户/权限/企业后端或整合现有 SpringBoot 系统

------

# 1. 前端（React / Web Audio API / WebSocket 或 WebRTC）

## 目标

- 负责录音、分片上传、展示实时文本、播放 TTS 音频，不直连任何 AI API（避免 key 泄露）。

## 关键选择与理由

- **WebSocket**（MediaRecorder 分片上传）——实现简单、兼容性好，适合 MVP。
- **WebRTC**（低延迟双向音频流）——若需要超低延迟并且要双向实时音频（多人/语音会议风格），选 WebRTC。初期用 WebSocket，后续按需升级到 WebRTC。

## 具体实现要点（前端）

1. **录音与分片**
   - 使用 `getUserMedia` + `MediaRecorder`（mime=audio/webm;codecs=opus）；
   - 分片（250ms）上传 Base64 或二进制（ArrayBuffer）到后端 WebSocket。
2. **UI**
   - 录音按钮（开始/暂停/结束），显示实时识别文本（interim transcript），显示角色 system message、情绪 slider、剧情节点选择（for storyteller）。
3. **播放音频**
   - 若后端返回音频 blob（base64），前端解码并播放（AudioContext.decodeAudioData）。
   - 可在浏览器端用 SpeechSynthesis 做 fallback（但音色/情绪有限）。
4. **连接与回落**
   - WebSocket -> 若连接失败降级到 HTTP 上传 + polling。
5. **安全**
   - 前端只用短期 session token（JWT），从后端获取并在 WebSocket 握手时发送。

## 接口示例（前端 → 后端）

- WebSocket Messages:
  - `{type: "audio-chunk", seq: 1, data: <binary>}`
  - `{type: "audio-end", final: true}`
- Receive:
  - `{type: "stt-interim", text: "..."}`
  - `{type: "llm-token", token: "..."}`
  - `{type: "tts-chunk", audio: <base64>}`

------

# 2. Gateway / Ingress（Nginx / Traefik）

## 目标

- 负责 TLS 终端、路由、WebSocket 支持、简单鉴权、负载均衡。

## 调整架构建议

- **反向代理配置**：Nginx 支持 `proxy_pass` WebSocket，需要头 `Upgrade`/`Connection`。Traefik 对 WebSocket 支持更原生，配合 Kubernetes Ingress 更方便。
- **SSL/TLS**：使用 Let’s Encrypt / cert-manager（K8s）自动管理证书。
- **健康检查**：配置 `/health` endpoints，gateway 根据健康路由到 Orchestrator 实例。
- **Sticky session**：WebSocket 连接需要粘性（session affinity）或使用 session 路由到正确后端（Redis 存 routing 信息）。

------

# 3. AI Orchestrator（核心：Python + FastAPI）

这是**AI 接口实际调用的地方**。下面按职责拆成子模块并给出实现细节。

## 子模块与职责

1. **Session 管理**
   - 存短期历史到 Redis（rolling window，按 token 限制），每 session ID 与 WebSocket 关联。
   - API: `POST /session/create` 返回 `session_id, jwt_token`；WebSocket 握手时附带 token。
2. **STT 接入模块**
   - 接收音频分片或最终文件，调用 STT（OpenAI Whisper 或音频-enabled LLM）。
   - 实现实时 interim transcript：流式转写并 send back interim.
   - 把 transcribed text 写入 session history。
3. **Prompt Builder / Persona 管理**
   - 每个角色存 JSON：`{id, name, system_prompt, default_voice, memory_policy}`。
   - Prompt Builder 合并：`system_prompt + retrieved RAG snippets + recent_history + user_transcript + assistant_instructions`。
   - 控制生成长度、温度（configurable per-role）。
4. **LLM 调用（流式）**
   - 调用 OpenAI streaming API（或 Anthropic stream），把 token 快速转发到前端（`llm-token`）。
   - 同时对生成文本做 **post-moderation filter**（禁止内容）。
   - 处理 stop sequences（例如用户命令 "stop"）。
5. **TTS 调用 / 音频生成**
   - 支持两种策略：
     - **Server-side TTS**：Orchestrator 调用 cloud TTS，将音频流传回前端（优点：可控 voice/SSML）；
     - **Client-side TTS**：后端只发文本，前端用 SpeechSynthesis（优点：实现快，缺点：声音单一）。
   - 对情绪/语气支持：为不同 emotions 设置 SSML 参数或选择不同 voice profiles。
6. **RAG / Memory**
   - 检索策略：当 user query 需回忆时（或 periodically）调用 embedding API -> vector DB -> top-k snippets -> 注入 prompt。
   - Memory policy: 规则化（explicit save by user 或 system-detected important statements）。
7. **Security / Moderation**
   - pre-check：对 user input 做敏感词检测（简单黑名单正则）。
   - post-check：对 LLM 输出调用 moderation API（若违规则用 safe-fallback）。
   - 日志但要脱敏（不记录 PII 或做加密）。

## 具体实现细节（代码范式）

- **FastAPI + WebSocket**，用 `async` 和 `httpx`（或官方 SDK）做流式转发。
- **流式转发实现**：
  - 向 LLM 发起 streaming call（保持单个 async generator），逐 token 发给前端；为保障稳定性，使用断点续传 / token buffer（防止同一批次消息过多）。
- **Prompt length & token 控制**：
  - 控制 `max_input_tokens = MODEL_LIMIT - expected_generation_tokens - margin`，在 prompt 超限时做摘要压缩（可用 LLM 做摘要）。

## 接口示例（后端）

- `POST /api/role` -> list roles
- `POST /api/session/{session_id}/save-memory` -> save snippet to vector DB
- WebSocket for audio & events (see 前端协议)

## 你（Python）要做的事情

- 实现 Orchestrator 主体（FastAPI app）、session/Redis、LLM streaming adapter、RAG glue code、prompt templates、moderation pipeline、TTS orchestrator。

------

# 4. 音频处理微服务（C++，gRPC）

## 目标

- 在音频进入 STT 前做高性能预处理：降噪（RNNoise / webrtc-audio-processing），VAD（Voice Activity Detection），opus 编码/解码，转码（采样率标准化），以及 optional real-time processing（AEC / AGC）。

## 为什么用 C++

- 实时音频对延迟敏感，且已有 libhv / libopus 经验，C++ 能提供低延迟与高吞吐。

## 微服务职责

1. **接收 raw audio（WebSocket 或 RTP via Gateway）**
2. **处理流程**：
   - Resample -> Mono -> VAD -> Trim silence -> Denoise -> opus encode
3. **输出**：
   - 分片发送到 Orchestrator（或直接给云 STT 支持 stream’ing format）
4. **接口**：
   - gRPC 服务（methods: `ProcessChunk`, `Flush`, `GetStats`）
   - 支持返回 metadata：`speech_probability`, `noise_level`，用于 Orchestrator 决策（是否触发 LLM call）

## 具体实现建议

- 使用 libopus / libsndfile / webrtc-audio-processing / rnnoise。
- 每个 audio session 分配一个处理 worker 池，避免主线程阻塞。
- 提供 health-check & metrics（Prometheus）。

## 你（C++）要做的事情

- 实现上述 gRPC 服务，接入 libopus + rnnoise，编写单元测试（自动化用 GoogleTest）。

------

# 5. LLM / STT / TTS 选型与接入细节（OpenAI 作为主）

## LLM

- **模型**：GPT-4o 系列（或 gpt-4o-mini）用于大多数聊天；对长篇生成或高准确度可请求 GPT-4。
- **接入点**：使用 OpenAI 官方 SDK（Python），并启用 streaming（SSE 或 WebSocket 代理），以便 token-by-token 返回。
- **策略**：
  - `temperature` 可 role-level 调整（苏格拉底 low temp 0.2；故事型 higher 0.7）；
  - 指令模板化（system, user, assistant messages）。

## STT

- **首选**：OpenAI Whisper / OpenAI audio endpoint（若用 OpenAI 全家桶最简单）。
- **实现**：
  - 对于流式转写：将 C++ 微服务或 Gateway 输出的 opus stream 转为 STT 要求的 format（16k PCM），并使用 streaming endpoint 获得 interim transcripts。

## TTS

- **方案 A（Server-side）**：OpenAI 的 TTS 模型或云 TTS（Amazon Polly / Azure）。优点：控制 voice、SSML、情绪。
- **方案 B（Client-side）**：浏览器 SpeechSynthesis（快速 MVP）。
- **建议**：MVP 用浏览器 TTS；产品化用 server-side TTS 支持情绪化声音（SSML）。

## 调用细节

- 为每次 LLM 调用保存 cost metrics（input tokens, output tokens），以便计费和限额。
- 在 Orchestrator 中实现 `LLMAdapter` 抽象，可切换 vendor（OpenAI / Anthropic）而不改业务代码。

------

# 6. 向量搜索 / RAG（RedisVector / Milvus / Pinecone）

## 职责

- 存储长期记忆、角色背景知识片段、剧情节点等，支持 top-k 检索注入 prompt。

## 建议

- **MVP**：RedisVector（易部署）或 Pinecone（托管）
- **实现细节**：
  1. 文本片段入库前做去重/合并（避免重复爆炸）。
  2. 每次检索限制 tokens 总和（例如 top-k snippets 总 token <= 1500）。
  3. 定期做 memory compaction（长时间未访问的记忆降权或归档）。
  4. 保存 metadata（user_id, role_id, timestamp, source）。

## 你（Python）要做

- 实现 embedding 调用 + 向量 DB 写入/检索接口 + memory policy。

------

# 7. 数据库与缓存（Postgres + Redis）

## Postgres

- 存储用户、角色定义、会话元数据、权限信息、日志索引（不存全文对话）。

## Redis

- Session cache（短期对话），路由粘性 info，限流 counters。

## Schema（建议）

- `users(id, username, email, hashed_password, created_at)`
- `roles(id, name, system_prompt, default_voice, config_json)`
- `sessions(id, user_id, role_id, state_json, last_active)`
- `memories(id, user_id, role_id, text, embedding_id, metadata)`（embedding 存向量 DB，表里存索引）

------

# 8. 部署 / Infra（Kubernetes 推荐）

## 最小可运行拓扑

- ingress (Traefik)
- gateway (stateless) replicas
- orchestrator (FastAPI) scaled by CPU, with concurrency limit
- audio-processing (C++) deployment with HPA based on CPU
- postgres, redis (stateful)
- vector db (Milvus / RedisVector)
- monitoring stack（Prometheus, Grafana, Loki）
- object storage（S3 / MinIO）用于录音持久化

## 具体调整点

- **Autoscaling**：LLM 调用受外部 API limit，内部服务 autoscale 需与 LLM 预算匹配（不要盲目扩容导致成本暴涨）。
- **Horizontal Pod Affinity / Anti-affinity**：把 audio-processing 服务调度到有较好网络/IO 的 node。
- **Persistent Volumes**：向量 DB 和 Postgres 使用 PVC。
- **Secrets**：通过 K8s Secret / Vault 存储 AI provider keys，保证仅 Orchestrator 可以访问。

------

# 9. 监控 / Logging / 报警

## 监控指标（必须）

- STT latency, LLM latency (input/output tokens), TTS latency
- WebSocket connection count, message rate
- Token usage / daily cost estimate
- Error rate, rejection rate（moderation）

## 报警

- 超过 token 日消耗阈值
- LLM 服务 5xx 或超时率上升

## 日志

- 对话日志需脱敏或加密，存储策略与 GDPR 合规（用户可删除）。

------

# 10. 安全 / 合规（务必写清楚）

1. **API keys** 存后端（K8s Secrets / Vault），前端不保存。
2. **用户隐私**：录音/对话加密存储，提供删除导出接口。
3. **内容审查**：组合 blacklist regex + LLM moderation（post-check），对违规内容自动替换或拒答。
4. **IP 合规**：对明显受版权保护角色（如哈利·波特）标注“fan-style”并尽早咨询法律。
5. **权限**：用户操作前用 OAuth2 / JWT 授权，敏感操作（导出数据）做额外验证。

------

# 11. 测试策略（含单元/集成/压力）

- **单元测试**：Prompt Builder、RAG pipeline、LLMAdapter mock。
- **集成测试**：用 fake LLM（mock server）验证 streaming token 转发、session 管理。
- **端到端测试**：CI pipeline 模拟录音上传 → STT → LLM → TTS 流程（可用 prerecorded audio）。
- **压力测试**：Webbench 或 wrk 测试 WebSocket 并发连接；注意并行 LLM 请求会受 API 限制，需模拟限流器。
- **用户测试**：A/B 测试不同 persona prompt，评估一致性与用户满意度。

------

# 12. 成本与限制考虑（简述）

- LLM token/voice API 成本高（按 token/min）。生产时需做预算、限额与缓存热门 responses。
- 向量 DB 成本（托管 vs 自托管）需衡量（Pinecone：简便；Milvus：自控）。
- 并发大时的瓶颈通常为 LLM 并发额度而非自身 infra。

------

# 13. 开发任务分解（按你现有技能分派）

- **你（C++）**：
  - 实现 audio-processing gRPC 服务（resample, VAD, denoise, opus encode）
  - 编写音频处理的单元测试与性能基准（latency, throughput）
  - 可选：实现 a small C++ client 工具用于本地调试音频 pipeline
- **你（Python）**：
  - 搭建 FastAPI Orchestrator：session 管理、LLMAdapter、STT/TTS glue、RAG 脚本
  - 实现 prompt templates + persona 管理（JSON），实现 streaming token 转发逻辑
  - 实装 moderation pipeline、metrics、日志与向量 DB 接入
- **你（Java，可选）**：
  - 如果需企业后台或结合学校日立项目：实现用户/权限服务（SpringBoot），或把现有 SpringBoot 接入 auth layer
- **前端（可由你或前端同学）**：
  - React 录音组件、WebSocket client、TTS 播放器、角色选择页

------

# 14. 交付件（我可以立刻为你生成）

你想让我现在直接输出下面哪一项的可运行样板？我会立刻把代码/配置贴出来：

- A. FastAPI Orchestrator 最小可运行样例（WebSocket audio chunks → mock LLM stream → TTS chunk 回传）。
- B. C++ audio-processing gRPC 服务代码骨架（libopus + rnnoise 示例）。
- C. React 前端录音 + WebSocket 交互组件（包含播放 base64 audio）。
- D. Persona JSON 模板（3 个角色：苏格拉底、哈利、福尔摩斯），包含 system_prompt 与 output schema。
- E. Kubernetes 部署清单（Ingress/Deployment/Service/HorizontalPodAutoscaler 的 YAML 模板）。